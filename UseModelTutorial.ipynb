{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8cc258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-07 16:15:50.828815: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from emoji import UNICODE_EMOJI\n",
    "import emoji\n",
    "import regex as re\n",
    "import tweepy\n",
    "from tqdm import tqdm\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import lightgbm as lgbm\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "# loading the tensorflow universal sentence encoder 4 model\n",
    "# embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embed = hub.load(\"universal-sentence-encoder_4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f27c5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup API auhorizaion\n",
    "\n",
    "# assign the values accordingly\n",
    "consumer_key = \"jfpVhd1zOxM6uqR1GjcQzEsns\"\n",
    "consumer_secret = \"c4PdCb2fvIbFRHlgQJlT6k0cPlRUtl1E0akP5dSvmKz1NFUHmr\"\n",
    "access_token = \"1250984376671670272-3wFUr43oEV9NUiPtv8f7ygVT74Rw3s\"\n",
    "access_token_secret = \"2OyDgcbZWYbVwgvFJttSIEroJzmOOTwq9hGMGmVeRs9mg\"\n",
    "\n",
    "# authorization of consumer key and consumer secret\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "  \n",
    "# set access to user's access key and access secret \n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb0e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is how you construct a Prediction Set. Provide a list of user ids and the function will build your dataset\n",
    "# save directory is where you would like your dataset to be saved to. \n",
    "# ex: r'/Users/lincolnrychecky/desktop/trainingDataBots.csv' saves the file as a csv named trainingDataBots.csv to my desktop\n",
    "\n",
    "def build_prediction_set(userIds, saveDirectory):\n",
    "    # construct the training set by calling predefined functions\n",
    "    for id in tqdm(userIds):\n",
    "        # Put in a try-except block in case a forbidden user id is in there\n",
    "        try:\n",
    "            print(id)\n",
    "            user = api.get_user(user_id=id)\n",
    "\n",
    "            recentFiveTweets = api.user_timeline(user_id=id, \n",
    "                                    # 200 is the maximum allowed count\n",
    "                                    count=11,\n",
    "                                    include_rts = True,\n",
    "                                    # Necessary to keep full_text \n",
    "                                    # otherwise only the first 140 words are extracted\n",
    "                                    tweet_mode = 'extended'\n",
    "                                    )\n",
    "            fullText = \"\"\n",
    "            for info in recentFiveTweets:\n",
    "                fullText = fullText + info.full_text + \" \"\n",
    "            cleanedText = clean(fullText)\n",
    "\n",
    "            ids.append(id)\n",
    "            emojis.append(split_count(fullText))\n",
    "            followers.append(getFollowers(user))\n",
    "            following.append(getFriends(user))\n",
    "            hashtags.append(countHashtags(fullText))\n",
    "            urls.append(countUrls(fullText))\n",
    "            statuses.append(getStatuses(user))\n",
    "            verified.append(getVerified(user))\n",
    "            text.append(cleanedText)\n",
    "            rawText.append(fullText)\n",
    "            human.append(0)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # turn dict into dataframe for export\n",
    "    trainDataBot = pd.DataFrame.from_dict(dataDict)\n",
    "    trainDataBot.to_csv(saveDirectory, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f6e32f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PoliticalBotAnalysis",
   "language": "python",
   "name": "politicalbotanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
